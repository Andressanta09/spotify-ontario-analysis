{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354e84af",
   "metadata": {},
   "source": [
    "# Data Analysis and Results Generation\n",
    "\n",
    "This notebook performs exploratory data analysis and generates the final results for the dashboard. We'll focus on identifying patterns and trends in the music data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b61cef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded CSV file: ../data/processed\\clean_tracks_20250919_120046.csv\n",
      "📊 Dataset shape: (6928, 37)\n",
      "Columns available: ['track_id', 'name', 'artist', 'album', 'popularity', 'duration_ms', 'playlist_id', 'duration_min', 'id', 'name_audio', 'artist_audio', 'album_audio', 'popularity_audio', 'duration_ms_audio', 'explicit', 'release_date', 'track_number', 'album_type', 'total_tracks', 'artist_id', 'album_id', 'duration_minutes', 'is_recent', 'is_single', 'preview_url', 'energy', 'valence', 'danceability', 'acousticness', 'instrumentalness', 'liveness', 'speechiness', 'tempo', 'loudness', 'key', 'mode', 'time_signature']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure directories\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "DASHBOARD_DIR = \"../dashboard\"\n",
    "\n",
    "# Ensure dashboard directory exists\n",
    "os.makedirs(DASHBOARD_DIR, exist_ok=True)\n",
    "\n",
    "def get_latest_file(pattern):\n",
    "    \"\"\"Get the most recent file matching the pattern.\"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files found matching {pattern}\")\n",
    "    return max(files, key=os.path.getctime)\n",
    "\n",
    "# Load processed data\n",
    "try:\n",
    "    # Try parquet first, then CSV\n",
    "    try:\n",
    "        tracks_file = get_latest_file(f\"{PROCESSED_DATA_DIR}/clean_tracks_*.parquet\")\n",
    "        tracks_df = pd.read_parquet(tracks_file)\n",
    "        print(f\"✅ Loaded parquet file: {tracks_file}\")\n",
    "    except FileNotFoundError:\n",
    "        tracks_file = get_latest_file(f\"{PROCESSED_DATA_DIR}/clean_tracks_*.csv\")\n",
    "        tracks_df = pd.read_csv(tracks_file)\n",
    "        print(f\"✅ Loaded CSV file: {tracks_file}\")\n",
    "    \n",
    "    print(f\"📊 Dataset shape: {tracks_df.shape}\")\n",
    "    print(f\"Columns available: {list(tracks_df.columns)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ No processed data found. Please run the cleaning notebook first.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d6444",
   "metadata": {},
   "source": [
    "## Top Tracks Analysis\n",
    "\n",
    "Identify the most popular tracks and their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55039526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎵 Analyzing top tracks...\n",
      "✅ Found 100 top tracks by popularity\n",
      "📋 Top tracks exported with columns: ['track_id', 'name', 'artist', 'album', 'popularity', 'release_date', 'danceability', 'energy', 'valence', 'duration_min']\n"
     ]
    }
   ],
   "source": [
    "# Analyze top tracks with available data\n",
    "print(\"🎵 Analyzing top tracks...\")\n",
    "\n",
    "# Get tracks with popularity data\n",
    "if 'popularity' in tracks_df.columns:\n",
    "    top_tracks = tracks_df.sort_values('popularity', ascending=False).head(100)\n",
    "    print(f\"✅ Found {len(top_tracks)} top tracks by popularity\")\n",
    "else:\n",
    "    # Fallback: use all tracks if no popularity column\n",
    "    top_tracks = tracks_df.head(100)\n",
    "    print(\"⚠️ No popularity column found - using first 100 tracks\")\n",
    "\n",
    "# Build available columns for top tracks\n",
    "available_cols = ['name', 'artist']\n",
    "optional_cols = ['album', 'popularity', 'release_date', 'release_year', \n",
    "                'danceability', 'energy', 'valence', 'duration_min']\n",
    "\n",
    "# Add ID column (could be 'id' or 'track_id')\n",
    "if 'track_id' in tracks_df.columns:\n",
    "    available_cols.insert(0, 'track_id')\n",
    "elif 'id' in tracks_df.columns:\n",
    "    available_cols.insert(0, 'id')\n",
    "\n",
    "# Add optional columns that exist\n",
    "for col in optional_cols:\n",
    "    if col in tracks_df.columns:\n",
    "        available_cols.append(col)\n",
    "\n",
    "# Create top tracks info with available columns\n",
    "top_tracks_info = top_tracks[available_cols].fillna('N/A').to_dict('records')\n",
    "print(f\"📋 Top tracks exported with columns: {available_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899b52d",
   "metadata": {},
   "source": [
    "## Feature Analysis\n",
    "\n",
    "Analyze correlations between different audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e6c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Analyzing feature correlations...\n",
      "✅ Correlation analysis completed for: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'popularity']\n",
      "📊 Feature matrix size: (10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature correlations with available data\n",
    "print(\"🔗 Analyzing feature correlations...\")\n",
    "\n",
    "# Define potential audio features\n",
    "potential_features = ['danceability', 'energy', 'loudness', 'speechiness', \n",
    "                     'acousticness', 'instrumentalness', 'liveness', 'valence', \n",
    "                     'tempo', 'popularity']\n",
    "\n",
    "# Get available features\n",
    "available_features = [col for col in potential_features if col in tracks_df.columns]\n",
    "\n",
    "if len(available_features) >= 2:\n",
    "    # Calculate correlation matrix for available features\n",
    "    correlation_matrix = tracks_df[available_features].corr()\n",
    "    \n",
    "    # Convert to dictionary for JSON\n",
    "    feature_correlations = correlation_matrix.round(3).to_dict('index')\n",
    "    \n",
    "    print(f\"✅ Correlation analysis completed for: {available_features}\")\n",
    "    print(f\"📊 Feature matrix size: {correlation_matrix.shape}\")\n",
    "else:\n",
    "    # Fallback: create empty correlation matrix\n",
    "    feature_correlations = {}\n",
    "    print(\"⚠️ Not enough numeric features for correlation analysis\")\n",
    "    print(f\"Available features: {available_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c0667",
   "metadata": {},
   "source": [
    "## Temporal Analysis\n",
    "\n",
    "Analyze trends in musical features over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f94f62b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Analyzing temporal trends...\n",
      "✅ Temporal analysis completed for: ['danceability', 'energy', 'valence', 'popularity']\n",
      "📈 Years covered: 1955.0 - 2025.0\n"
     ]
    }
   ],
   "source": [
    "# Temporal analysis with available data\n",
    "print(\"📅 Analyzing temporal trends...\")\n",
    "\n",
    "# Check if we have date information\n",
    "date_col = None\n",
    "if 'release_year' in tracks_df.columns:\n",
    "    date_col = 'release_year'\n",
    "elif 'release_date' in tracks_df.columns:\n",
    "    # Extract year from release_date if needed\n",
    "    tracks_df['release_year'] = pd.to_datetime(tracks_df['release_date'], errors='coerce').dt.year\n",
    "    date_col = 'release_year'\n",
    "\n",
    "if date_col is not None:\n",
    "    # Define features for temporal analysis\n",
    "    temporal_features = ['danceability', 'energy', 'valence', 'popularity']\n",
    "    available_temporal_features = [col for col in temporal_features if col in tracks_df.columns]\n",
    "    \n",
    "    if available_temporal_features:\n",
    "        # Calculate yearly averages\n",
    "        yearly_stats = tracks_df.groupby(date_col)[available_temporal_features].mean()\n",
    "        \n",
    "        # Convert to dashboard format\n",
    "        yearly_trends = yearly_stats.reset_index().round(3).to_dict('records')\n",
    "        \n",
    "        print(f\"✅ Temporal analysis completed for: {available_temporal_features}\")\n",
    "        print(f\"📈 Years covered: {tracks_df[date_col].min()} - {tracks_df[date_col].max()}\")\n",
    "    else:\n",
    "        yearly_trends = []\n",
    "        print(\"⚠️ No numeric features available for temporal analysis\")\n",
    "else:\n",
    "    yearly_trends = []\n",
    "    print(\"⚠️ No date information available for temporal analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab6ac5",
   "metadata": {},
   "source": [
    "## Generate data.json File\n",
    "\n",
    "Generate the data.json file that will be used by the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af227061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Generating dashboard data...\n",
      "✅ Dashboard data saved: ../dashboard/data.json\n",
      "📊 Data includes:\n",
      "   - 100 top tracks\n",
      "   - 10 feature correlations\n",
      "   - 66 yearly trend points\n",
      "   - 19 feature statistics\n",
      "   - Total dataset: 6928 tracks\n",
      "\n",
      "🎉 Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive dashboard data\n",
    "print(\"💾 Generating dashboard data...\")\n",
    "\n",
    "# Ensure all required variables exist\n",
    "if 'top_tracks_info' not in locals():\n",
    "    top_tracks_info = []\n",
    "if 'feature_correlations' not in locals():\n",
    "    feature_correlations = {}\n",
    "if 'yearly_trends' not in locals():\n",
    "    yearly_trends = []\n",
    "if 'available_features' not in locals():\n",
    "    available_features = []\n",
    "\n",
    "# Create comprehensive dashboard data\n",
    "dashboard_data = {\n",
    "    'metadata': {\n",
    "        'total_tracks': len(tracks_df),\n",
    "        'available_features': available_features,\n",
    "        'columns': list(tracks_df.columns),\n",
    "        'date_range': {\n",
    "            'min_year': None,\n",
    "            'max_year': None\n",
    "        },\n",
    "        'last_updated': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    'top_tracks': top_tracks_info,\n",
    "    'feature_correlations': feature_correlations,\n",
    "    'yearly_trends': yearly_trends,\n",
    "    'summary_stats': {}\n",
    "}\n",
    "\n",
    "# Add date range if available\n",
    "if 'release_year' in tracks_df.columns and tracks_df['release_year'].notna().any():\n",
    "    dashboard_data['metadata']['date_range']['min_year'] = int(tracks_df['release_year'].min())\n",
    "    dashboard_data['metadata']['date_range']['max_year'] = int(tracks_df['release_year'].max())\n",
    "\n",
    "# Add summary statistics for numeric columns\n",
    "numeric_cols = tracks_df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if col not in ['track_id', 'id'] and not col.endswith('_audio'):  # Skip ID and duplicate columns\n",
    "        try:\n",
    "            col_data = tracks_df[col].dropna()  # Remove NaN values\n",
    "            if len(col_data) > 0:\n",
    "                dashboard_data['summary_stats'][col] = {\n",
    "                    'mean': float(round(col_data.mean(), 3)),\n",
    "                    'median': float(round(col_data.median(), 3)),\n",
    "                    'std': float(round(col_data.std(), 3)),\n",
    "                    'min': float(round(col_data.min(), 3)),\n",
    "                    'max': float(round(col_data.max(), 3)),\n",
    "                    'count': int(len(col_data))\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not calculate stats for {col}: {e}\")\n",
    "\n",
    "# Save JSON file\n",
    "try:\n",
    "    output_path = f\"{DASHBOARD_DIR}/data.json\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dashboard_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Dashboard data saved: {output_path}\")\n",
    "    print(f\"📊 Data includes:\")\n",
    "    print(f\"   - {len(top_tracks_info)} top tracks\")\n",
    "    print(f\"   - {len(feature_correlations)} feature correlations\")\n",
    "    print(f\"   - {len(yearly_trends)} yearly trend points\")\n",
    "    print(f\"   - {len(dashboard_data['summary_stats'])} feature statistics\")\n",
    "    print(f\"   - Total dataset: {len(tracks_df)} tracks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving dashboard data: {e}\")\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    \n",
    "    # Save backup in processed data directory\n",
    "    try:\n",
    "        backup_path = f\"{PROCESSED_DATA_DIR}/dashboard_data_backup.json\"\n",
    "        with open(backup_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dashboard_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"💾 Backup saved: {backup_path}\")\n",
    "    except Exception as backup_error:\n",
    "        print(f\"❌ Could not save backup: {backup_error}\")\n",
    "        # Last resort: print data structure\n",
    "        print(\"📋 Dashboard data structure:\")\n",
    "        for key, value in dashboard_data.items():\n",
    "            if isinstance(value, (list, dict)):\n",
    "                print(f\"   {key}: {type(value).__name__} with {len(value)} items\")\n",
    "            else:\n",
    "                print(f\"   {key}: {type(value).__name__}\")\n",
    "\n",
    "print(\"\\n🎉 Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
