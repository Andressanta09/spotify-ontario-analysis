{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354e84af",
   "metadata": {},
   "source": [
    "# Data Analysis and Results Generation\n",
    "\n",
    "This notebook performs exploratory data analysis and generates the final results for the dashboard. We'll focus on identifying patterns and trends in the music data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b61cef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded CSV file: ../data/processed\\clean_tracks_20250919_120046.csv\n",
      "üìä Dataset shape: (6928, 37)\n",
      "Columns available: ['track_id', 'name', 'artist', 'album', 'popularity', 'duration_ms', 'playlist_id', 'duration_min', 'id', 'name_audio', 'artist_audio', 'album_audio', 'popularity_audio', 'duration_ms_audio', 'explicit', 'release_date', 'track_number', 'album_type', 'total_tracks', 'artist_id', 'album_id', 'duration_minutes', 'is_recent', 'is_single', 'preview_url', 'energy', 'valence', 'danceability', 'acousticness', 'instrumentalness', 'liveness', 'speechiness', 'tempo', 'loudness', 'key', 'mode', 'time_signature']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure directories\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "DASHBOARD_DIR = \"../dashboard\"\n",
    "\n",
    "# Ensure dashboard directory exists\n",
    "os.makedirs(DASHBOARD_DIR, exist_ok=True)\n",
    "\n",
    "def get_latest_file(pattern):\n",
    "    \"\"\"Get the most recent file matching the pattern.\"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files found matching {pattern}\")\n",
    "    return max(files, key=os.path.getctime)\n",
    "\n",
    "# Load processed data\n",
    "try:\n",
    "    # Try parquet first, then CSV\n",
    "    try:\n",
    "        tracks_file = get_latest_file(f\"{PROCESSED_DATA_DIR}/clean_tracks_*.parquet\")\n",
    "        tracks_df = pd.read_parquet(tracks_file)\n",
    "        print(f\"‚úÖ Loaded parquet file: {tracks_file}\")\n",
    "    except FileNotFoundError:\n",
    "        tracks_file = get_latest_file(f\"{PROCESSED_DATA_DIR}/clean_tracks_*.csv\")\n",
    "        tracks_df = pd.read_csv(tracks_file)\n",
    "        print(f\"‚úÖ Loaded CSV file: {tracks_file}\")\n",
    "    \n",
    "    print(f\"üìä Dataset shape: {tracks_df.shape}\")\n",
    "    print(f\"Columns available: {list(tracks_df.columns)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå No processed data found. Please run the cleaning notebook first.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d6444",
   "metadata": {},
   "source": [
    "## Top Tracks Analysis\n",
    "\n",
    "Identify the most popular tracks and their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55039526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Analyzing top tracks...\n",
      "‚úÖ Found 100 top tracks by popularity\n",
      "üìã Top tracks exported with columns: ['track_id', 'name', 'artist', 'album', 'popularity', 'release_date', 'danceability', 'energy', 'valence', 'duration_min']\n"
     ]
    }
   ],
   "source": [
    "# Analyze top tracks with available data\n",
    "print(\"üéµ Analyzing top tracks...\")\n",
    "\n",
    "# Get tracks with popularity data\n",
    "if 'popularity' in tracks_df.columns:\n",
    "    top_tracks = tracks_df.sort_values('popularity', ascending=False).head(100)\n",
    "    print(f\"‚úÖ Found {len(top_tracks)} top tracks by popularity\")\n",
    "else:\n",
    "    # Fallback: use all tracks if no popularity column\n",
    "    top_tracks = tracks_df.head(100)\n",
    "    print(\"‚ö†Ô∏è No popularity column found - using first 100 tracks\")\n",
    "\n",
    "# Build available columns for top tracks\n",
    "available_cols = ['name', 'artist']\n",
    "optional_cols = ['album', 'popularity', 'release_date', 'release_year', \n",
    "                'danceability', 'energy', 'valence', 'duration_min']\n",
    "\n",
    "# Add ID column (could be 'id' or 'track_id')\n",
    "if 'track_id' in tracks_df.columns:\n",
    "    available_cols.insert(0, 'track_id')\n",
    "elif 'id' in tracks_df.columns:\n",
    "    available_cols.insert(0, 'id')\n",
    "\n",
    "# Add optional columns that exist\n",
    "for col in optional_cols:\n",
    "    if col in tracks_df.columns:\n",
    "        available_cols.append(col)\n",
    "\n",
    "# Create top tracks info with available columns\n",
    "top_tracks_info = top_tracks[available_cols].fillna('N/A').to_dict('records')\n",
    "print(f\"üìã Top tracks exported with columns: {available_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899b52d",
   "metadata": {},
   "source": [
    "## Feature Analysis\n",
    "\n",
    "Analyze correlations between different audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e6c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Analyzing feature correlations...\n",
      "‚úÖ Correlation analysis completed for: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'popularity']\n",
      "üìä Feature matrix size: (10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature correlations with available data\n",
    "print(\"üîó Analyzing feature correlations...\")\n",
    "\n",
    "# Define potential audio features\n",
    "potential_features = ['danceability', 'energy', 'loudness', 'speechiness', \n",
    "                     'acousticness', 'instrumentalness', 'liveness', 'valence', \n",
    "                     'tempo', 'popularity']\n",
    "\n",
    "# Get available features\n",
    "available_features = [col for col in potential_features if col in tracks_df.columns]\n",
    "\n",
    "if len(available_features) >= 2:\n",
    "    # Calculate correlation matrix for available features\n",
    "    correlation_matrix = tracks_df[available_features].corr()\n",
    "    \n",
    "    # Convert to dictionary for JSON\n",
    "    feature_correlations = correlation_matrix.round(3).to_dict('index')\n",
    "    \n",
    "    print(f\"‚úÖ Correlation analysis completed for: {available_features}\")\n",
    "    print(f\"üìä Feature matrix size: {correlation_matrix.shape}\")\n",
    "else:\n",
    "    # Fallback: create empty correlation matrix\n",
    "    feature_correlations = {}\n",
    "    print(\"‚ö†Ô∏è Not enough numeric features for correlation analysis\")\n",
    "    print(f\"Available features: {available_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c0667",
   "metadata": {},
   "source": [
    "## Temporal Analysis\n",
    "\n",
    "Analyze trends in musical features over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f94f62b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Analyzing temporal trends...\n",
      "‚úÖ Temporal analysis completed for: ['danceability', 'energy', 'valence', 'popularity']\n",
      "üìà Years covered: 1955.0 - 2025.0\n"
     ]
    }
   ],
   "source": [
    "# Temporal analysis with available data\n",
    "print(\"üìÖ Analyzing temporal trends...\")\n",
    "\n",
    "# Check if we have date information\n",
    "date_col = None\n",
    "if 'release_year' in tracks_df.columns:\n",
    "    date_col = 'release_year'\n",
    "elif 'release_date' in tracks_df.columns:\n",
    "    # Extract year from release_date if needed\n",
    "    tracks_df['release_year'] = pd.to_datetime(tracks_df['release_date'], errors='coerce').dt.year\n",
    "    date_col = 'release_year'\n",
    "\n",
    "if date_col is not None:\n",
    "    # Define features for temporal analysis\n",
    "    temporal_features = ['danceability', 'energy', 'valence', 'popularity']\n",
    "    available_temporal_features = [col for col in temporal_features if col in tracks_df.columns]\n",
    "    \n",
    "    if available_temporal_features:\n",
    "        # Calculate yearly averages\n",
    "        yearly_stats = tracks_df.groupby(date_col)[available_temporal_features].mean()\n",
    "        \n",
    "        # Convert to dashboard format\n",
    "        yearly_trends = yearly_stats.reset_index().round(3).to_dict('records')\n",
    "        \n",
    "        print(f\"‚úÖ Temporal analysis completed for: {available_temporal_features}\")\n",
    "        print(f\"üìà Years covered: {tracks_df[date_col].min()} - {tracks_df[date_col].max()}\")\n",
    "    else:\n",
    "        yearly_trends = []\n",
    "        print(\"‚ö†Ô∏è No numeric features available for temporal analysis\")\n",
    "else:\n",
    "    yearly_trends = []\n",
    "    print(\"‚ö†Ô∏è No date information available for temporal analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab6ac5",
   "metadata": {},
   "source": [
    "## Generate data.json File\n",
    "\n",
    "Generate the data.json file that will be used by the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af227061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Generating dashboard data...\n",
      "‚úÖ Dashboard data saved: ../dashboard/data.json\n",
      "üìä Data includes:\n",
      "   - 100 top tracks\n",
      "   - 10 feature correlations\n",
      "   - 66 yearly trend points\n",
      "   - 19 feature statistics\n",
      "   - Total dataset: 6928 tracks\n",
      "\n",
      "üéâ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive dashboard data\n",
    "print(\"üíæ Generating dashboard data...\")\n",
    "\n",
    "# Ensure all required variables exist\n",
    "if 'top_tracks_info' not in locals():\n",
    "    top_tracks_info = []\n",
    "if 'feature_correlations' not in locals():\n",
    "    feature_correlations = {}\n",
    "if 'yearly_trends' not in locals():\n",
    "    yearly_trends = []\n",
    "if 'available_features' not in locals():\n",
    "    available_features = []\n",
    "\n",
    "# Create comprehensive dashboard data\n",
    "dashboard_data = {\n",
    "    'metadata': {\n",
    "        'total_tracks': len(tracks_df),\n",
    "        'available_features': available_features,\n",
    "        'columns': list(tracks_df.columns),\n",
    "        'date_range': {\n",
    "            'min_year': None,\n",
    "            'max_year': None\n",
    "        },\n",
    "        'last_updated': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    'top_tracks': top_tracks_info,\n",
    "    'feature_correlations': feature_correlations,\n",
    "    'yearly_trends': yearly_trends,\n",
    "    'summary_stats': {}\n",
    "}\n",
    "\n",
    "# Add date range if available\n",
    "if 'release_year' in tracks_df.columns and tracks_df['release_year'].notna().any():\n",
    "    dashboard_data['metadata']['date_range']['min_year'] = int(tracks_df['release_year'].min())\n",
    "    dashboard_data['metadata']['date_range']['max_year'] = int(tracks_df['release_year'].max())\n",
    "\n",
    "# Add summary statistics for numeric columns\n",
    "numeric_cols = tracks_df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if col not in ['track_id', 'id'] and not col.endswith('_audio'):  # Skip ID and duplicate columns\n",
    "        try:\n",
    "            col_data = tracks_df[col].dropna()  # Remove NaN values\n",
    "            if len(col_data) > 0:\n",
    "                dashboard_data['summary_stats'][col] = {\n",
    "                    'mean': float(round(col_data.mean(), 3)),\n",
    "                    'median': float(round(col_data.median(), 3)),\n",
    "                    'std': float(round(col_data.std(), 3)),\n",
    "                    'min': float(round(col_data.min(), 3)),\n",
    "                    'max': float(round(col_data.max(), 3)),\n",
    "                    'count': int(len(col_data))\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not calculate stats for {col}: {e}\")\n",
    "\n",
    "# Save JSON file\n",
    "try:\n",
    "    output_path = f\"{DASHBOARD_DIR}/data.json\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dashboard_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Dashboard data saved: {output_path}\")\n",
    "    print(f\"üìä Data includes:\")\n",
    "    print(f\"   - {len(top_tracks_info)} top tracks\")\n",
    "    print(f\"   - {len(feature_correlations)} feature correlations\")\n",
    "    print(f\"   - {len(yearly_trends)} yearly trend points\")\n",
    "    print(f\"   - {len(dashboard_data['summary_stats'])} feature statistics\")\n",
    "    print(f\"   - Total dataset: {len(tracks_df)} tracks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving dashboard data: {e}\")\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    \n",
    "    # Save backup in processed data directory\n",
    "    try:\n",
    "        backup_path = f\"{PROCESSED_DATA_DIR}/dashboard_data_backup.json\"\n",
    "        with open(backup_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dashboard_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üíæ Backup saved: {backup_path}\")\n",
    "    except Exception as backup_error:\n",
    "        print(f\"‚ùå Could not save backup: {backup_error}\")\n",
    "        # Last resort: print data structure\n",
    "        print(\"üìã Dashboard data structure:\")\n",
    "        for key, value in dashboard_data.items():\n",
    "            if isinstance(value, (list, dict)):\n",
    "                print(f\"   {key}: {type(value).__name__} with {len(value)} items\")\n",
    "            else:\n",
    "                print(f\"   {key}: {type(value).__name__}\")\n",
    "\n",
    "print(\"\\nüéâ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
